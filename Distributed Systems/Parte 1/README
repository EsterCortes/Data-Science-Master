NOTA SOBRE LOS FICHEROS DE TWEETS DESCARGADOS

Debido al peso de uno de los ficheros de los tweets no se han podido incluir aquí. En el fichero Tweets
figura un enlace desde donde se pueden descargar dichos ficheros. 
El resultado que figura en la memoria corresponde al archivo de 8GB, los otros dos simplemente se han usado para 
pruebas iniciales y para comparar el tiempo en cuanto a escalabilidad.

PASOS PARA LA EJECUCIÓN

1. Colocar todos los ficheros en la misma ruta

2. Descargar los tweets mediante python twitterstream.py -o -> tweets.json

3. Tener instalada la librería mrjob mediante pip install mrjob

4. Para ejecutar inline: python MRJobTwitter.py -r inline tweets.json --file AFINN-111.txt

5. Para ejecutar local: python MRJobTwitter.py -r local tweets.json --file AFINN-111.txt

6. Para ejecutar con Hadoop:
	6.1. Subir a hdfs los ficheros de entrada: hdfs dfs -put tweets.json /practica (si el directorio no está creado, 
	crearlo mediante hdfs dfs -mkdir practica)
	6.2. Ejecutar: python MRJobTwitter.py -r hadoop hdfs:///user/root/practica/tweets.json --file AFINN-111.txt

EN CASO DE PROBLEMAS DE PERMISOS EN HADOOP (COMO ME OCURRIÓ A MI)

Previo. Usando como base ubuntu y teniendo instalado docker

1. En local, colocar todos los ficheros en la misma ruta

2. Descargar la imagen de Cloudera mediante docker pull cloudera/quickstart:latest

3. Subir los ficheros al contenedor de Cloudera: docker cp fichero_a_subir cloudera:/home/Cloudera/Documents

4. Ejecutar Cloudera mediante docker run --hostname=quickstart.cloudera --privileges=true -t -i -p 8888:8888 -p 80:80 
--name cloudera cloudera quickstart /usr/bin/docker-quickstart

5. Situados en la consola de Cloudera, instalar la librería mrjob mediante pip install mrjob

6. Movernos hasta la ruta donde tenemos nuestros ficheros. Podremos realizar los pasos indicados en la primera parte 
del 4-6. 
